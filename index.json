[{"authors":null,"categories":null,"content":"Pursuing a PhD in Augmented Vision focusing on enhancing human visual capabilities through computational optics. Passionate about researching to contribute to people’s lives.\n","date":1770940800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1770940800,"objectID":"25efc737ff2b3e8a140c1a04499f0b33","permalink":"https://ar-ai.org/author/cristian-rendon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cristian-rendon/","section":"authors","summary":"Pursuing a PhD in Augmented Vision focusing on enhancing human visual capabilities through computational optics. Passionate about researching to contribute to people’s lives.","tags":null,"title":"Cristian Rendon","type":"authors"},{"authors":null,"categories":null,"content":"I am currently pursuing a PhD in Artificial Intelligence with a focus on advancing high-quality, real-time image generation. My research builds on a strong foundation in computer vision, having previously contributed to various projects at Picsart AI Research (PAIR). These projects include image outpainting/inpainting, texture generation, style transfer, text-to-image generation, and text-to-multiview generation, all aimed at pushing the boundaries of AI-driven visual creativity.\n","date":1756771200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756771200,"objectID":"7ba7d4e5b5393de15795fd1e735af7b7","permalink":"https://ar-ai.org/author/hovhannes-margaryan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hovhannes-margaryan/","section":"authors","summary":"I am currently pursuing a PhD in Artificial Intelligence with a focus on advancing high-quality, real-time image generation. My research builds on a strong foundation in computer vision, having previously contributed to various projects at Picsart AI Research (PAIR).","tags":null,"title":"Hovhannes Margaryan","type":"authors"},{"authors":null,"categories":null,"content":"Exploring the research world in the spirit of Alexander von Humboldt. Publishing on Augmented Reality since 2000. Born and raised in Munich (Germany). Abroad since graduation in 2005. Read more on my personal homepage\n","date":1752105600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1752105600,"objectID":"a96325e743d044df6670ca4f8a7b3395","permalink":"https://ar-ai.org/author/christian-sandor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christian-sandor/","section":"authors","summary":"Exploring the research world in the spirit of Alexander von Humboldt. Publishing on Augmented Reality since 2000. Born and raised in Munich (Germany). Abroad since graduation in 2005. Read more on my personal homepage","tags":null,"title":"Christian Sandor","type":"authors"},{"authors":null,"categories":null,"content":"I am a Master’s student interested in immersive technologies, human-computer interaction, multisensory perception and communication. In the lab, my project explores various 3D visualization techniques for surgical applications. Outside of academia, I enjoy knitting and hiking.\n","date":1739836800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1739836800,"objectID":"69060e1904e852d16dc1f7bda669a8a3","permalink":"https://ar-ai.org/author/zofia-samsel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zofia-samsel/","section":"authors","summary":"I am a Master’s student interested in immersive technologies, human-computer interaction, multisensory perception and communication. In the lab, my project explores various 3D visualization techniques for surgical applications. Outside of academia, I enjoy knitting and hiking.","tags":null,"title":"Zofia Samsel","type":"authors"},{"authors":null,"categories":null,"content":"Born and raised in Kanagawa (Japan). Currently a master’s student in Japan. My hobbies: Cheerleading and Travel.\n","date":17388e5,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":17388e5,"objectID":"dc85ebaf48112f5aa4d987dec9d22be2","permalink":"https://ar-ai.org/author/mizuki-akiyama/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mizuki-akiyama/","section":"authors","summary":"Born and raised in Kanagawa (Japan). Currently a master’s student in Japan. My hobbies: Cheerleading and Travel.","tags":null,"title":"Mizuki Akiyama","type":"authors"},{"authors":null,"categories":null,"content":"Research Engineer and member of the National Artificial Intelligence Research Program Network (PNRIA/CNRS). I work closely with research teams, supporting their AI-related projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"34272b7b8685374beddb076a5eb9e2d8","permalink":"https://ar-ai.org/author/cyrille-leroux/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cyrille-leroux/","section":"authors","summary":"Research Engineer and member of the National Artificial Intelligence Research Program Network (PNRIA/CNRS). I work closely with research teams, supporting their AI-related projects.","tags":null,"title":"Cyrille Leroux","type":"authors"},{"authors":null,"categories":null,"content":"My name is Daniel Filonik and I design and implement interaction environments for immersive data exploration and collaboration. My work utilizes a wide range of technologies, like interactive screens, projections, augmented and virtual reality headsets. My goal is to create environments that enable people to become immersed in data and gain new insights. I have worked on a variety of applications ranging from expert systems for researchers and data scientists to interactive exhibits that facilitate science communication with general audiences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"755ef3b37dc09f2f4d33d721b15975f6","permalink":"https://ar-ai.org/author/daniel-filonik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daniel-filonik/","section":"authors","summary":"My name is Daniel Filonik and I design and implement interaction environments for immersive data exploration and collaboration. My work utilizes a wide range of technologies, like interactive screens, projections, augmented and virtual reality headsets.","tags":null,"title":"Daniel Filonik","type":"authors"},{"authors":null,"categories":null,"content":"PhD student in Augmented Reality. With the interest of enhancing human performance using spatial intelligent systems to support remote communication and collaboration for creatives and designers.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7f8de235c54bd43f288e5d6c2085fa90","permalink":"https://ar-ai.org/author/david-maruscsak/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-maruscsak/","section":"authors","summary":"PhD student in Augmented Reality. With the interest of enhancing human performance using spatial intelligent systems to support remote communication and collaboration for creatives and designers.","tags":null,"title":"Dávid Maruscsák","type":"authors"},{"authors":null,"categories":null,"content":"I am a PhD student exploring the potential use of cutting-edge technologies such as Artificial Intelligence (AI) and Virtual Reality (VR) in educational contexts. My research primarily focuses on integrating these technologies into training environments to create new and innovative ways to develop soft skills that are crucial in the modern era, both in academic and professional pursuits. Outside of academia, I have a passion for traveling and discovering new cultures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"565557854b18794a17853c879f4e5f6b","permalink":"https://ar-ai.org/author/esra-caki/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/esra-caki/","section":"authors","summary":"I am a PhD student exploring the potential use of cutting-edge technologies such as Artificial Intelligence (AI) and Virtual Reality (VR) in educational contexts. My research primarily focuses on integrating these technologies into training environments to create new and innovative ways to develop soft skills that are crucial in the modern era, both in academic and professional pursuits.","tags":null,"title":"Esra Caki","type":"authors"},{"authors":null,"categories":null,"content":"Master’s student in Human-Computer Interaction and Design, focused on designing intuitive interfaces that enable creative and meaningful interactions between people and technology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8e0e742eaa1e0ea24105f341040016bc","permalink":"https://ar-ai.org/author/federica-zanchi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/federica-zanchi/","section":"authors","summary":"Master’s student in Human-Computer Interaction and Design, focused on designing intuitive interfaces that enable creative and meaningful interactions between people and technology.","tags":null,"title":"Federica Zanchi","type":"authors"},{"authors":null,"categories":null,"content":"PhD student in Augmented Reality and interactive content generation. Using AR and AI to support lighting design for exhibitions through intuitive visual tools.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8dd629479c15fa9eae9577e028784a78","permalink":"https://ar-ai.org/author/francesco-dettori/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/francesco-dettori/","section":"authors","summary":"PhD student in Augmented Reality and interactive content generation. Using AR and AI to support lighting design for exhibitions through intuitive visual tools.","tags":null,"title":"Francesco Dettori","type":"authors"},{"authors":null,"categories":null,"content":"Focusing on enhancing user interaction and experience in immersive systems, and exploring collaboration using virtual and augmented reality. Read more on my personal homepage\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3aaa49fe88d230f4888d3e4df1e0833","permalink":"https://ar-ai.org/author/huyen-nguyen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huyen-nguyen/","section":"authors","summary":"Focusing on enhancing user interaction and experience in immersive systems, and exploring collaboration using virtual and augmented reality. Read more on my personal homepage","tags":null,"title":"Huyen Nguyen","type":"authors"},{"authors":null,"categories":null,"content":"Finishing my last year of my Bachelor’s Degree, I am really excited about Artificial Intelligence and exploring the domain of Computer Vision. Outside my studies and research, I enjoy Powerlifting!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9cb6422916216f5bd2a0f7b062e17c73","permalink":"https://ar-ai.org/author/martin-leiva/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/martin-leiva/","section":"authors","summary":"Finishing my last year of my Bachelor’s Degree, I am really excited about Artificial Intelligence and exploring the domain of Computer Vision. Outside my studies and research, I enjoy Powerlifting!","tags":null,"title":"Martín Leiva","type":"authors"},{"authors":null,"categories":null,"content":"I am currently pursuing a Master’s degree in Digital Sciences at Université Paris Cité, France. With a background in both Biotechnology and Data Science, I have a strong interest in interdisciplinary research, particularly in the applications of Artificial Intelligence in Life Sciences and Social Sciences.\nAdditionally, I have over 5 years of experience working in Vietnam’s startup Business, primarily in AI or Education. During my current internship in France, I am contributing to the Standbyme project, which integrates AI and Virtual Reality for conflict management. Beyond my academic and professional pursuits, I enjoy reading and performing music.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"40ebf63d8863c72238e4773485dc4d56","permalink":"https://ar-ai.org/author/pham-hai-nam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pham-hai-nam/","section":"authors","summary":"I am currently pursuing a Master’s degree in Digital Sciences at Université Paris Cité, France. With a background in both Biotechnology and Data Science, I have a strong interest in interdisciplinary research, particularly in the applications of Artificial Intelligence in Life Sciences and Social Sciences.","tags":null,"title":"Pham Hai Nam","type":"authors"},{"authors":null,"categories":null,"content":"I am pursuing a PhD in Artificial Intelligence on the topic of real-time visual generation and its 3D consistency. This is part of a broader goal of achieving real-time generation/reconstruction for augmented reality. As such, I am interested in many cutting-edge fields, particularly spatio-temporal memory and optimal transport.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"490b77a7c3c4c08bba4fc346615f184d","permalink":"https://ar-ai.org/author/rodolphe-nonclercq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rodolphe-nonclercq/","section":"authors","summary":"I am pursuing a PhD in Artificial Intelligence on the topic of real-time visual generation and its 3D consistency. This is part of a broader goal of achieving real-time generation/reconstruction for augmented reality.","tags":null,"title":"Rodolphe Nonclercq","type":"authors"},{"authors":null,"categories":null,"content":"AI research engineer at CNRS and based at IRISA. I support AI research projects using the Jean-Zay supercomputer and organize training courses.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c5915bb7c8c46cbe27d7ec26461a74d5","permalink":"https://ar-ai.org/author/thomas-betton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/thomas-betton/","section":"authors","summary":"AI research engineer at CNRS and based at IRISA. I support AI research projects using the Jean-Zay supercomputer and organize training courses.","tags":null,"title":"Thomas Betton","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://ar-ai.org/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Cristian Rendon"],"categories":null,"content":"I am pleased to invite you to the defense of my PhD thesis titled:\nEnhancing Human Performance Through Augmented Vision\nsupervised by: Professeur Christian SANDOR, Maître de conférences Richard LEGRAS et Maître de conférences Marie-Anne BURCKLEN.\nThe defense will be held in English and will take place on Monday, February 16th, 2026 at 14:00.\nLocation: Room: Salle Conférences - Campus Universitaire bât.507 - Rue du Belvédère 91405 - Orsay.\nOnline: https://bbb.lisn.upsaclay.fr/rooms/n3k-chl-cpk-ghs/join\nJury: M. Vincent NOURRIT, Professeur, IMT Atlantique, Rapporteur Mme Stefanie ZOLLMANN, Associate Professor, Aarhus University, Rapporteure M. Laurent VABRE, Docteur, Imagine Eyes, Examinateur M. Gabriele FACCIOLO, Professeur, ENS Paris-Saclay, Examinateur Keywords: Augmented Vision, Human-Computer Interaction, Human Visual System, Freeform Lenses, Spatial Light Modulator, Computational Optics\nAbstract: Augmented Vision (AV) is an emerging field that aims to enhance human sight by directly modulating a user’s view of the real world. Unlike traditional visual aids such as spectacles or contact lenses that provide static correction, AV systems offer dynamic, real-time modulation of the real world. This capability to actively change the user’s view opens up new possibilities for both correcting and augmenting human vision. One of the three most common applications for AV is vision correction. This is particularly relevant for presbyopia, an age-related condition affecting 1.8 billion people worldwide. Conventional solutions, such as progressive glasses or multifocal contact lenses, offer a static compromise with fixed focal zones or averaged focus, respectively. In contrast, AV systems use their inherently dynamic behaviour to address these limitations. For instance, some systems use automatic single focus approaches using focus-tunable lenses to focus at specific depths. However, this approach is limited to applying one optical power across the entire field of view at a time. The alternatives are multifocal systems, which provide granular, spatial control over focus. However, while the concept has been demonstrated, there is a lack of objective evaluation of the imaging quality of such systems.\nThis thesis addresses two primary objectives. The first is to establish a conceptual framework for Augmented Vision by stating a formal definition and a new taxonomy that categorises existing systems by their light modulation method. The second is to address a gap identified by this framework: the lack of multifocal augmented vision systems and the evaluation of their imaging performance. To tackle the challenges of providing multifocal vision, this thesis proposes an optical architecture that combines a phase-only Spatial Light Modulator (SLM) with a Lohmann lens, allowing spatial control of the optical power over the Field Of View (FOV), providing see-through, real-world modulation. We validate the prototype by measuring the Modulation Transfer Function (MTF) for global and multifocal correction, placing objects at various depths and setting the system’s focus at them. We use a camera with its focus set to infinity to simulate a presbyopic eye during the measurements. The results confirm that the system can bring objects from multiple depths into focus, demonstrating a higher MTF for each focused plane when the system is active compared to the inactive condition.\nThis thesis provides two primary contributions: (1) a formal conceptual framework for AV systems that identifies the research gap, and (2) a prototype of an optical platform for multifocal vision and its evaluation. This platform serves as foundational technology for exploring a new class of interfaces based on spatially-variant optical power to enhance visual perception and address widespread visual impairments.\n","date":1770940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770940800,"objectID":"2d4ef02921c45daf040d9e53a628f7bf","permalink":"https://ar-ai.org/post/26-13-02-cristian-defense/","publishdate":"2026-02-13T00:00:00Z","relpermalink":"/post/26-13-02-cristian-defense/","section":"post","summary":"I am pleased to invite you to the defense of my PhD thesis titled:\nEnhancing Human Performance Through Augmented Vision\nsupervised by: Professeur Christian SANDOR, Maître de conférences Richard LEGRAS et Maître de conférences Marie-Anne BURCKLEN.","tags":null,"title":"Cristian's PhD Defense","type":"post"},{"authors":null,"categories":null,"content":"Evaluate Neural Rendering Methods for XR/AI Applications​ Keywords Computer Graphics, Augmented Reality, Neural Rendering, Generative AI, Rapid Prototyping\nProject Surpervisors Daniel Filonik, Christian Sandor, Huyen Nguyen.\nEmails: daniel.filonik@universite-paris-saclay.fr, christian.sandor@universite-paris-saclay.fr, thi-thuong-huyen.nguyen@universite-paris-saclay.fr\nDescription The recent advances in machine learning have inspired and enabled an abundance of novel methods in computer graphics. Collectively these approaches are sometimes referred to as “neural rendering methods”. Applications of these methods are vast, and they can outperform classic shaders in fidelity and performance. For example, they include:\nGeometry/Texture Compression Generative Materials Generative Geometries Light Simulation/Physically Based Rendering Post-Processing/Effects The fundamental idea behind neural rendering is: Rather than compute an exact solution for an expensive computation, use machine learning to approximate it. This way, the goal is that computations which were previously too expensive for real-time applications, can now be performed more efficiently.\nEfficiency is especially important in resource constrained computing environment, such as portable extended reality (XR) devices. Due to the engineering challenges in portable devices, they are often limited in terms of their compute capabilities. At the same time, XR applications have very high requirements in terms of accuracy and realism. The better the quality of the rendering, the more convincing and seamless the integration between virtual and real-world objects.\nThrough this project, we want to explore novel rendering approaches and foster expertise in our team. As part of this, the intern will:\nReview the influx of novel neural rendering methods. Evaluate the feasibility of implementing these methods for XR/AI applications. In the process of this project, the intern will produce proof-of-concept prototypes to explore the viability of neural rendering methods for XR/AI applications. The goal is to implement two or more comparable methods, and rigorously test and compare their performance. This will allow the intern to gather valuable experience working with XR/AI technologies. Furthermore, we anticipate the publication of a literature review, which will provide the foundation for future research, and a first prototype to demonstrate the research direction.\nThe goal is to produce a literature review to cover the current state-of-the-art of neural rendering methods. This will be a valuable resource for researchers who are looking to get an overview of the rapidly growing research area. It will also allow the research intern to gain experience with the process of academic publishing.\nApply To express your interest, please send your application materials to Daniel Filonik, cc Huyen Nguyen, Christian Sandor:\nCV Transcript of Records Link to portfolio (github, personal homepage, etc.) ","date":1762992e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1762992e3,"objectID":"961760d4ffbbd4891b43d74981618763","permalink":"https://ar-ai.org/offers/internship-neural-rendering/","publishdate":"2025-11-13T00:00:00Z","relpermalink":"/offers/internship-neural-rendering/","section":"offers","summary":"Internship position at ARAI research team","tags":null,"title":"Internship (Neural Rendering)","type":"offers"},{"authors":["Hovhannes Margaryan"],"categories":null,"content":"On 28 March 2025, I had the pleasure of participating in ADOS in Paris—a creative technology event—co‑organized by Banodoco and Lightricks at Artifex Lab. Walking into that space, you could feel the excitement: artists, engineers, and technologists converging around the open‑source LTXV model. I’d been following LTXV’s rapid evolution—a text-to-video model that generates video clips from text in real time—so it was thrilling to see the community around it in action.\nFirst Day at ADOS: Talks \u0026amp; Connections In the evening’s lineup, standout presentations ranged from Emma Catnip’s soulful art pieces to Vibeke Bertelsen’s uncanny human forms, and Yvann Barbot’s ComfyUI audio-reactive video demos. My supervisor, Christian Sandor, also delivered an inspiring keynote, and our team showcased a demo exploring the convergence of AR and AI—a project that deeply influenced my own thinking about generative storytelling.\nAmid all this, I found LTXV’s open‑source availability fascinating—not just as technical innovation, but as a platform for collective creativity. Seeing Lightricks being open with their weights and tools seemed like a pivotal moment in democratizing video AI.\nDay Two: Hackathon \u0026amp; Video Editing and Reference-based Style‑Transfer Experiments The very next day I joined the ADOS hackathon, working side by side with others to push LTXV’s limits. Provided computation resources, power, snacks, and steady energy pulses fueled the intense collaborative rhythm of the event. For my project, I focused on one core idea: simultaneous style transfer and editing for text‑to‑video (T2V) using LTXV. Given a style reference image, a video clip, and a text prompt describing the edit, I experimented with three distinct methods to transfer visual style while\nLatent alpha blending: Alpha blending between the inverted style image latent and the inverted latent of the video’s first frame. KV sharing with alpha blending: Repeating the style’s keys and values to match video dimensions (repeat-interleave) and blending them. KV sharing via concatenation: Appending the style’s keys and values to the video’s keys and values for joint processing with full 3D attention. Your browser does not support video. Input Video Reference Style Method 1 Method 2 Method 3 (Layer 5) Method 3 (Layer 27) Text Prompt: A big horse triumphantly at the peak of a towering mountain. Panorama of rugged peaks and valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, moons, but the remainder of the scene is mostly realistic.\nFigure 1: Method 1 fails to propagate style information to the generated video. In Method 3, the choice of attention layer for injection critically impacts the outcome: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, whereas later-layer injection (e.g., layer 27) achieves a more balanced integration of style and motion. Style image from Pinterest. Input video from Hugging Face. For the text‑based editing component, I used RF‑Edit. My findings indicate that latent alpha blending is ineffective for style transfer. KV sharing methods can propagate style, but often at the expense of content fidelity or motion consistency. In particular, style injection may introduce unnatural motion not present in the original video. The injection point within the attention layers significantly affects results: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, while later-layer injection (e.g., layer 27) offers a better balance between style and motion.\nReflections \u0026amp; What Comes Next ADOS was an inspiring gathering—not only for witnessing the technical evolution of generative video, but also for experiencing the vibrant culture of open, collaborative experimentation that surrounds it. Huge thanks to the ADOS team—Banodoco, Lightricks, and Artifex Lab—for organizing an inspiring and technically rich weekend. Events like this make open‑source generative video feel alive. I can’t wait to keep building on what started in Paris.\n","date":1756771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756771200,"objectID":"68dd1840cc36b3e61a37eae0f0daf30b","permalink":"https://ar-ai.org/post/25-03-28-ados/","publishdate":"2025-09-02T00:00:00Z","relpermalink":"/post/25-03-28-ados/","section":"post","summary":"On 28 March 2025, I had the pleasure of participating in ADOS in Paris—a creative technology event—co‑organized by Banodoco and Lightricks at Artifex Lab. Walking into that space, you could feel the excitement: artists, engineers, and technologists converging around the open‑source LTXV model.","tags":null,"title":"My Experience at ADOS: From Keynotes to Hackathon Experiments","type":"post"},{"authors":["Zofia Samsel"],"categories":null,"content":"Hi, my name is Zofia. Originally from Poland, I am a M2 Master’s student at Université Paris Cité.\nFrom March 2023 to January 2024, I had the opportunity to be part of the ARAI Team as an intern. I joined the lab to work on a project in collaboration with La Chaire Innovation BOPA at Paul Brousse Hospital. I worked under the supervision of Prof. Nguyen Huyen, Prof. Christian Sandor, and Dr Clément Cormi (BOPA). Our focus was on using augmented reality to visualize 3D imaging during open liver surgery.\nDuring my time in the lab, I spent time at the hospital observing surgeries and designing experimental protocols to test how different ways of presenting 3D images impact surgical performance and communication. I also developed a demo app to present 3D reconstructions of the liver using the Canon MREAL HMD, which was later tested by surgeons. We also collaborated with PRIM 3D, a 3D printing lab, to create life-sized silicone liver models, which were later used in the experiment.\nThe project was a challenging yet rewarding experience that gave me valuable insight into how interdisciplinary collaboration between medical and technical expertise can lead to innovative solutions. It showed me how teamwork can help tackle complex tasks and contribute to improving healthcare.\nI was also lucky to attend the Young Researchers in Human-Computer Interaction Meeting in La Rochelle and volunteer at the SUI Symposium on Spatial User Interaction. Both events introduced me to new ideas, cutting-edge research, and inspiring people in the field of HCI.\nBut my internship wasn’t just about research! I met so many interesting people in the lab and had some great experiences outside the lab too. We organized fun outings like a trip to Asterix Park, or ice skating, which helped me feel more connected to the team.\nLooking back, my time with the ARAI Team was a valuable learning experience, both professionally and personally. I’m grateful for the support of my supervisors and colleagues. This experience has shaped my future, and I’m excited to see where it leads next!\n","date":1739836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739836800,"objectID":"090de10538cc1ca413fccb7f98de8a5a","permalink":"https://ar-ai.org/post/25-02-18-zofia-internship/","publishdate":"2025-02-18T00:00:00Z","relpermalink":"/post/25-02-18-zofia-internship/","section":"post","summary":"Hi, my name is Zofia. Originally from Poland, I am a M2 Master’s student at Université Paris Cité.\nFrom March 2023 to January 2024, I had the opportunity to be part of the ARAI Team as an intern.","tags":null,"title":"Zofia's Internship Report","type":"post"},{"authors":["Mizuki Akiyama"],"categories":null,"content":"Hello, this is Mizuki Akiyama. I’m a master’s student of Ochanomizu University in Japan. I joined team ARAI as an intern for about 2 months, from early October to mid December 2024. Not only did I proceed with my project, but I also met amazing people, so I was able to spend quality time. I will continue to work with team ARAI remotely from Tokyo!\nBefore coming, I had some online meetings with Professor Christian. Since we talked about how we would do the project during the meetings, I started the implementation immediately upon joining. I could focus very well on my project. The project is based on my personal experience as a cheerleader; I want to help cheerleaders to improve their training using XR technologies! Our ultimate goal is to do it with AR, but as a first step, I built a prototype with VR\nFurthermore, we went out for dinner together in Paris, and on another day, I was shown around the city. I had a great time with everyone during the other moments.\nWe had French Japanese food. I tried something called sushi, but it was not real sushi.\nAdditionally, I visited Paris every weekend and I had some trips in Europe. I found it fascinating to experience the different vibes of each country.\nEven though my English is not good, everyone was very welcoming. I’m so glad to experience this internship, and I appreciate their kindness. Please come visit Japan! Thank you very much, ありがとう！\n","date":17388e5,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":17388e5,"objectID":"6136817da3cca6d6ecadf9f199192271","permalink":"https://ar-ai.org/post/25-01-09-mizuki-internship/","publishdate":"2025-02-06T00:00:00Z","relpermalink":"/post/25-01-09-mizuki-internship/","section":"post","summary":"Hello, this is Mizuki Akiyama. I’m a master’s student of Ochanomizu University in Japan. I joined team ARAI as an intern for about 2 months, from early October to mid December 2024.","tags":null,"title":"Mizuki's Internship Report","type":"post"},{"authors":null,"categories":null,"content":"","date":1736258400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1736258400,"objectID":"32be030db0bebe88e8fea5f7a0d8579b","permalink":"https://ar-ai.org/post/25-01-07-offer/","publishdate":"2025-01-07T14:00:00Z","relpermalink":"/post/25-01-07-offer/","section":"post","summary":"We have a postdoc position available, to be filled ASAP. Interested? Click here to read more","tags":["new"],"title":"Postdoc Position Available","type":"post"},{"authors":null,"categories":null,"content":"Canon Japan visited us to present their AR strategy, including their current and future headsets. They also showed several really nice demos. It was fun as you can see in the image gallery below. You can also see Emmanuel Pietriga, leader of team ILDA, in some of the photos.\n","date":1729209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729209600,"objectID":"26c5963484a299e6991f328c265a899d","permalink":"https://ar-ai.org/post/24-10-18-canon/","publishdate":"2024-10-18T00:00:00Z","relpermalink":"/post/24-10-18-canon/","section":"post","summary":"Canon Japan visited us to present a talk about their new headesets. They also brought some interesting demos. It was fun!","tags":null,"title":"Visit by Canon Japan","type":"post"},{"authors":null,"categories":null,"content":"At the end of July 2024, we showed a demonstration at the premiere computer graphics conference: ACM SIGGRAPH. We even managed to get into one of their most challenging categories: Real-Time Live!\nClick to read Chris’s detailled experience report.\n","date":1726012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726012800,"objectID":"f68ceb1c9579e819dc975be6d3a97754","permalink":"https://ar-ai.org/post/24-09-11-rtl/","publishdate":"2024-09-11T00:00:00Z","relpermalink":"/post/24-09-11-rtl/","section":"post","summary":"At the end of July 2024, we showed a demonstration at the premiere computer graphics conference: ACM SIGGRAPH. We even managed to get into one of their most challenging categories: Real-Time Live!\nClick to read Chris’s detailled experience report.\n","tags":null,"title":"Team ARAI presents at ACM SIGGRAPH 2024 Real-Time Live!","type":"post"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://ar-ai.org/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ar-ai.org/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"https://ar-ai.org/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":null,"categories":null,"content":"Postdocs Summary Starting date: continuously Salary: up to EUR 4291 (gross per month; depending on experience) Introduction Generative Artiﬁcial Intelligence (AI) and Augmented Reality (AR) have made incredible progress over the last few years. We are not far away from a future, in which a user’s view could be continuously modified by their smart glasses. Besides some obvious ethical concerns about such a future, there are also immense opportunities for using this capability to make our lives better. The high-level context of this position is to help us develop fundamental technologies and study their applications and effects on humans.\nA special benefit of this position is that is intended to support outstanding young researchers to prepare an application for a lifetime position at CNRS. These positions are unique, as they provide the probably fastest tracks for talented young researchers to get a tenured position (more than 5 years faster than a comparable track in e.g. the USA). CNRS positions also come with a status as French civil servant, which includes significant benefits (e.g. sabbaticals with a duration of up to 8 years, true lifetime employment, an attractive pension, etc.).\nThe ideal candidate has 2-3 years of postdoc experience.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a4675e5414b26a1c709ae5bb576ebfb8","permalink":"https://ar-ai.org/offers/postdoc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/offers/postdoc/","section":"offers","summary":"Postdocs Summary Starting date: continuously Salary: up to EUR 4291 (gross per month; depending on experience) Introduction Generative Artiﬁcial Intelligence (AI) and Augmented Reality (AR) have made incredible progress over the last few years.","tags":null,"title":"Postdoc","type":"offers"}]