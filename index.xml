<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Team ARAI</title><link>https://ar-ai.org/</link><atom:link href="https://ar-ai.org/index.xml" rel="self" type="application/rss+xml"/><description>Team ARAI</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 04 Sep 2024 00:00:00 +0000</lastBuildDate><image><url>https://ar-ai.org/media/icon_huf3b78d476b7845bee05ab0b9b7b31b8f_51310_512x512_fill_lanczos_center_3.png</url><title>Team ARAI</title><link>https://ar-ai.org/</link></image><item><title>Example Event</title><link>https://ar-ai.org/event/example/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://ar-ai.org/event/example/</guid><description>&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://docs.hugoblox.com/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://docs.hugoblox.com/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Open Position - Internship (Lighting Design)</title><link>https://ar-ai.org/post/internship-museology/</link><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/internship-museology/</guid><description>&lt;h1 id="can-you-manipulate-light-the-way-you-imagine-it-exploring-interfaces-for-exhibition-lighting-design">“Can You Manipulate Light the Way You Imagine It?”: Exploring Interfaces for Exhibition Lighting Design&lt;/h1>
&lt;h2 id="keywords">Keywords&lt;/h2>
&lt;p>Interaction Design, Participatory Design, Lighting Design, Generative AI, Human-AI collaboration&lt;/p>
&lt;h2 id="project-surpervisors">Project Surpervisors&lt;/h2>
&lt;p>&lt;a href="https://ar-ai.org/author/francesco-dettori/" target="_blank" rel="noopener">Francesco Dettori&lt;/a>, &lt;a href="https://ar-ai.org/author/christian-sandor/" target="_blank" rel="noopener">Christian Sandor&lt;/a>, &lt;a href="https://ar-ai.org/author/huyen-nguyen/" target="_blank" rel="noopener">Huyen Nguyen&lt;/a>.&lt;/p>
&lt;p>Emails: &lt;a href="mailto:francesco.dettori@universite-paris-saclay.fr">francesco.dettori@universite-paris-saclay.fr&lt;/a>, &lt;a href="mailto:christian.sandor@universite-paris-saclay.fr">christian.sandor@universite-paris-saclay.fr&lt;/a>, &lt;a href="mailto:thi-thuong-huyen.nguyen@universite-paris-saclay.fr">thi-thuong-huyen.nguyen@universite-paris-saclay.fr&lt;/a>&lt;/p>
&lt;h2 id="description">Description&lt;/h2>
&lt;p>This internship is part of MuseoXR, a CNRS-funded project that investigates new tools for collaborative exhibition lighting design. A preliminary study with curators, scenographers, and lighting designers revealed three recurring challenges:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Different ways of describing light:&lt;/strong> Professionals from different fields use distinct vocabularies, narrative, perceptual, or technical, making it hard to align creative and practical decisions.&lt;/li>
&lt;li>&lt;strong>Design and tool desynchronization:&lt;/strong> Designs evolve so rapidly that models, photometric simulations, and pre-visualizations often fall out of sync. As a result, these tools are rarely used in practice despite their potential value.&lt;/li>
&lt;li>&lt;strong>Manual trial-and-error:&lt;/strong> To translate a creative vision into a real lighting setup, designers must manually browse and test hundreds of technical files.&lt;/li>
&lt;/ul>
&lt;p>Existing tools like DIALux enable precise photometric simulation but are too slow and rigid for early creative exploration. AI-based tools can manipulate lighting in images, but still lack the precision and controllability that professionals require. Recent advances in generative AI make lighting control directly in 2D images possible [Magar, 2025; Careaga, 2025], while new rendering methods now support accurate simulation of real luminaires using IES data [Gadia, 2024]. These technologies establish a strong foundation, but the interface challenge remains.&lt;/p>
&lt;p>This raises a key HCI question: &lt;strong>How can we design an interface that allows users with different expertise to communicate and manipulate lighting ideas naturally, and in a way that can be translated into measurable photometric data?&lt;/strong>&lt;/p>
&lt;p>At the ARAI team, we are developing a system that reconstructs lighting configurations from sketches, descriptions, or photos - the image below illustrates the concept.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-internship-museology" href="https://ar-ai.org/media/albums/internship-museology/image.png" >
&lt;img src="https://ar-ai.org/media/albums/internship-museology/image_hu60a6cb4c5f3dfeaba4422d02c2da1dee_1409826_750x750_fit_q75_h2_lanczos_3.webp" loading="lazy" alt="image.png" width="750" height="291">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>Preliminary observations show that non-experts draw fan shapes, professionals use cones and numeric parameters, and curators describe effects through words like “glowing” or “alive.” This diversity reveals a gap between how people think about light and how existing systems represent it.&lt;/p>
&lt;p>&lt;strong>This internship focuses on the design and evaluation of interfaces that help users express lighting ideas across backgrounds and expertise levels.&lt;/strong>&lt;/p>
&lt;p>The intern will:&lt;/p>
&lt;ul>
&lt;li>Conduct &lt;strong>participatory and comparative design sessions&lt;/strong> with both professionals and
non-experts. These sessions will include &lt;strong>collective workshops&lt;/strong> to observe how ideas are negotiated, and individual tasks to explore personal strategies.&lt;/li>
&lt;li>Analyze the &lt;strong>ways people use drawing, speech, gestures, and image references&lt;/strong> to describe lighting conditions or desired effects.&lt;/li>
&lt;li>Develop &lt;strong>high-fidelity prototypes&lt;/strong> of sketch-based, text-based, or combined interfaces that allow users to express lighting without working in 3D.&lt;/li>
&lt;li>Iterate and evaluate the prototypes through &lt;strong>user testing&lt;/strong>, assessing clarity, control, and how effectively the system can convert input into structured lighting data.&lt;/li>
&lt;/ul>
&lt;p>The final prototype will serve as a “front-end” to our AI system, acting as a &lt;strong>shared language&lt;/strong> between users and models, one that can transform abstract ideas into photometrically accurate and physically realizable lighting configurations. It will contribute directly to faster, more accessible lighting workflows for museum design teams.&lt;/p>
&lt;h2 id="relevant-skills-to-have">Relevant skills to have:&lt;/h2>
&lt;ul>
&lt;li>Solid foundation in Human-Computer Interaction and user-centered design.&lt;/li>
&lt;li>Experience with interface prototyping (Figma, Adobe XD, or code-based).&lt;/li>
&lt;li>Interest in participatory design and qualitative methods.&lt;/li>
&lt;li>Familiarity with generative AI for images or visual computing is a plus.&lt;/li>
&lt;li>Good communication skills in English.&lt;/li>
&lt;/ul>
&lt;h2 id="expected-outcome">Expected outcome:&lt;/h2>
&lt;p>To be defined in collaboration with the team. Likely deliverables include a structured design study, interface prototypes, and guidelines for intuitive lighting manipulation tools integrated into the MuseoXR platform.&lt;/p>
&lt;h2 id="apply">Apply&lt;/h2>
&lt;p>To express your interest, please send your application materials to Francesco Dettori, cc Huyen Nguyen, Christian Sandor:&lt;/p>
&lt;ul>
&lt;li>CV&lt;/li>
&lt;li>Transcript of Records&lt;/li>
&lt;li>Link to portfolio (github, personal homepage, etc.)&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;p>&lt;strong>[Careaga, 2025]&lt;/strong> Careaga, C. and Aksoy, Y. (2025). Physically Controllable Relighting of Photographs. SIGGRAPH Conference Papers, 1–10.&lt;/p>
&lt;p>&lt;strong>[Gadia, 2024]&lt;/strong> Gadia, D., Lombardo, V., Maggiorini, D., Natilla, A. (2024). Implementing many-lights rendering with IES-based lights. Applied Sciences, 14(3), p.1022.&lt;/p>
&lt;p>&lt;strong>[Magar, 2025]&lt;/strong> Magar, N., Hertz, A., Tabellion, E., Pritch, Y., Rav-Acha, A., Shamir, A., Hoshen, Y. (2025). LightLab: Controlling light sources in images with diffusion models. SIGGRAPH Conference Papers, 1–11.&lt;/p></description></item><item><title>Open Position - Internship (Neural Rendering)</title><link>https://ar-ai.org/post/internship-neural-rendering/</link><pubDate>Thu, 13 Nov 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/internship-neural-rendering/</guid><description>&lt;h1 id="evaluate-neural-rendering-methods-for-xrai-applications">Evaluate Neural Rendering Methods for XR/AI Applications​&lt;/h1>
&lt;h2 id="keywords">Keywords&lt;/h2>
&lt;p>Computer Graphics, Augmented Reality, Neural Rendering, Generative AI, Rapid Prototyping&lt;/p>
&lt;h2 id="project-surpervisors">Project Surpervisors&lt;/h2>
&lt;p>&lt;a href="https://ar-ai.org/author/daniel-filonik/" target="_blank" rel="noopener">Daniel Filonik&lt;/a>, &lt;a href="https://ar-ai.org/author/christian-sandor/" target="_blank" rel="noopener">Christian Sandor&lt;/a>, &lt;a href="https://ar-ai.org/author/huyen-nguyen/" target="_blank" rel="noopener">Huyen Nguyen&lt;/a>.&lt;/p>
&lt;p>Emails: &lt;a href="mailto:daniel.filonik@universite-paris-saclay.fr">daniel.filonik@universite-paris-saclay.fr&lt;/a>, &lt;a href="mailto:christian.sandor@universite-paris-saclay.fr">christian.sandor@universite-paris-saclay.fr&lt;/a>, &lt;a href="mailto:thi-thuong-huyen.nguyen@universite-paris-saclay.fr">thi-thuong-huyen.nguyen@universite-paris-saclay.fr&lt;/a>&lt;/p>
&lt;h2 id="description">Description&lt;/h2>
&lt;p>The recent advances in machine learning have inspired and enabled an abundance of novel methods in computer graphics. Collectively these approaches are sometimes referred to as &amp;ldquo;neural rendering methods&amp;rdquo;. Applications of these methods are vast, and they can outperform classic shaders in fidelity and performance. For example, they include:&lt;/p>
&lt;ul>
&lt;li>Geometry/Texture Compression&lt;/li>
&lt;li>Generative Materials&lt;/li>
&lt;li>Generative Geometries&lt;/li>
&lt;li>Light Simulation/Physically Based Rendering&lt;/li>
&lt;li>Post-Processing/Effects&lt;/li>
&lt;/ul>
&lt;p>The fundamental idea behind neural rendering is: Rather than compute an exact solution for an expensive computation, use machine learning to approximate it. This way, the goal is that computations which were previously too expensive for real-time applications, can now be performed more efficiently.&lt;/p>
&lt;p>Efficiency is especially important in resource constrained computing environment, such as portable extended reality (XR) devices. Due to the engineering challenges in portable devices, they are often limited in terms of their compute capabilities. At the same time, XR applications have very high requirements in terms of accuracy and realism. The better the quality of the rendering, the more convincing and seamless the integration between virtual and real-world objects.&lt;/p>
&lt;p>Through this project, we want to explore novel rendering approaches and foster expertise in our team. As part of this, the intern will:&lt;/p>
&lt;ul>
&lt;li>Review the influx of novel neural rendering methods.&lt;/li>
&lt;li>Evaluate the feasibility of implementing these methods for XR/AI applications.&lt;/li>
&lt;/ul>
&lt;p>In the process of this project, the intern will produce proof-of-concept prototypes to explore the viability of neural rendering methods for XR/AI applications. The goal is to implement two or more comparable methods, and rigorously test and compare their performance. This will allow the intern to gather valuable experience working with XR/AI technologies. Furthermore, we anticipate the publication of a literature review, which will provide the foundation for future research, and a first prototype to demonstrate the research direction.&lt;/p>
&lt;p>The goal is to produce a literature review to cover the current state-of-the-art of neural rendering methods. This will be a valuable resource for researchers who are looking to get an overview of the rapidly growing research area. It will also allow the research intern to gain experience with the process of academic publishing.&lt;/p>
&lt;h2 id="apply">Apply&lt;/h2>
&lt;p>To express your interest, please send your application materials to Daniel Filonik, cc Huyen Nguyen, Christian Sandor:&lt;/p>
&lt;ul>
&lt;li>CV&lt;/li>
&lt;li>Transcript of Records&lt;/li>
&lt;li>Link to portfolio (github, personal homepage, etc.)&lt;/li>
&lt;/ul></description></item><item><title>My Experience at ADOS: From Keynotes to Hackathon Experiments</title><link>https://ar-ai.org/post/25-03-28-ados/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-03-28-ados/</guid><description>&lt;p>On &lt;strong>28 March 2025&lt;/strong>, I had the pleasure of participating in &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> in Paris—a creative technology event—co‑organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banodoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a> at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>. Walking into that space, you could feel the excitement: artists, engineers, and technologists converging around the open‑source &lt;a href="https://github.com/Lightricks/LTX-Video" target="_blank" rel="noopener">LTXV&lt;/a> model. I’d been following LTXV’s rapid evolution—a text-to-video model that generates video clips from text in real time—so it was thrilling to see the community around it in action.&lt;/p>
&lt;h2 id="first-day-at-ados-talks--connections">First Day at ADOS: Talks &amp;amp; Connections&lt;/h2>
&lt;p>In the evening’s lineup, standout presentations ranged from &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?t=468s" target="_blank" rel="noopener">Emma Catnip&lt;/a>’s soulful art pieces to &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Vibeke Bertelsen&lt;/a>’s uncanny human forms, and Yvann Barbot’s ComfyUI audio-reactive video demos. My supervisor, &lt;a href="https://drsandor.net" target="_blank" rel="noopener">Christian Sandor&lt;/a>, also delivered an inspiring keynote, and our team showcased a &lt;a href="https://youtu.be/gBeZSbaxMvc?t=7066" target="_blank" rel="noopener">demo&lt;/a> exploring the convergence of AR and AI—a project that deeply influenced my own thinking about generative storytelling.&lt;/p>
&lt;p>Amid all this, I found LTXV’s open‑source availability fascinating—not just as technical innovation, but as a platform for collective creativity. Seeing Lightricks being open with their weights and tools seemed like a pivotal moment in democratizing video AI.&lt;/p>
&lt;h2 id="day-two-hackathon--video-editing-and-reference-based-styletransfer-experiments">Day Two: Hackathon &amp;amp; Video Editing and Reference-based Style‑Transfer Experiments&lt;/h2>
&lt;p>The very next day I joined the ADOS hackathon, working side by side with others to push LTXV’s limits. Provided computation resources, power, snacks, and steady energy pulses fueled the intense collaborative rhythm of the event.
For my project, I focused on one core idea: simultaneous style transfer and editing for text‑to‑video (T2V) using LTXV. Given a style reference image, a video clip, and a text prompt describing the edit, I experimented with three distinct methods to transfer visual style while&lt;/p>
&lt;ol>
&lt;li>Latent alpha blending: Alpha blending between the inverted style image latent and the inverted latent of the video&amp;rsquo;s first frame.&lt;/li>
&lt;li>KV sharing with alpha blending: Repeating the style’s keys and values to match video dimensions (repeat-interleave) and blending them.&lt;/li>
&lt;li>KV sharing via concatenation: Appending the style&amp;rsquo;s keys and values to the video&amp;rsquo;s keys and values for joint processing with full 3D attention.&lt;/li>
&lt;/ol>
&lt;figure>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 60%; margin-bottom:0;">
&lt;video style="margin: 0" controls autoplay muted loop>
&lt;source src="media/input_hf.mp4" type="video/mp4">
Your browser does not support video.
&lt;/video>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Input Video &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 40%; margin-bottom:0;">
&lt;img src="media/style_image.png"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Reference Style &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 50%; margin-bottom:0;">
&lt;img src="media/algo_1.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 1 &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 50%; margin-bottom:0;">
&lt;img src="media/algo_2.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 2 &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 50%; margin-bottom: 10px;">
&lt;img src="media/algo_3_1.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 3 (Layer 5) &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 50%; margin-bottom: 10px;">
&lt;img src="media/algo_3_2.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 3 (Layer 27) &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;p style="font-size: 12px"> Text Prompt: A big horse triumphantly at the peak of a towering mountain. Panorama of rugged peaks and valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, moons, but the remainder of the scene is mostly realistic.&lt;/p>
&lt;figcaption style="text-align: left; font-style: italic;"> &lt;b>Figure 1:&lt;/b> Method 1 fails to propagate style information to the generated video. In Method 3, the choice of attention layer for injection critically impacts the outcome: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, whereas later-layer injection (e.g., layer 27) achieves a more balanced integration of style and motion. Style image from &lt;a href="https://www.pinterest.com">Pinterest.&lt;/a> Input video from &lt;a href="https://huggingface.co">Hugging Face.&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;p>For the text‑based editing component, I used &lt;a href="https://rf-solver-edit.github.io" target="_blank" rel="noopener">RF‑Edit&lt;/a>. My findings indicate that latent alpha blending is ineffective for style transfer. KV sharing methods can propagate style, but often at the expense of content fidelity or motion consistency. In particular, style injection may introduce unnatural motion not present in the original video. The injection point within the attention layers significantly affects results: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, while later-layer injection (e.g., layer 27) offers a better balance between style and motion.&lt;/p>
&lt;h2 id="reflections--what-comes-next">Reflections &amp;amp; What Comes Next&lt;/h2>
&lt;p>ADOS was an inspiring gathering—not only for witnessing the technical evolution of generative video, but also for experiencing the vibrant culture of open, collaborative experimentation that surrounds it.
Huge thanks to the ADOS team—&lt;strong>Banodoco, Lightricks, and Artifex Lab&lt;/strong>—for organizing an inspiring and technically rich weekend. Events like this make open‑source generative video feel alive. I can’t wait to keep building on what started in Paris.&lt;/p></description></item><item><title>Zofia's Internship Report</title><link>https://ar-ai.org/post/25-02-18-zofia-internship/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-02-18-zofia-internship/</guid><description>&lt;p>Hi, my name is Zofia. Originally from Poland, I am a &lt;strong>M2 Master’s student at Université Paris Cité&lt;/strong>.&lt;/p>
&lt;p>From March 2023 to January 2024, I had the opportunity to be part of the &lt;strong>ARAI Team&lt;/strong> as an intern. I joined the lab to work on a project in collaboration with &lt;a href="https://www.chaire-bopa.fr/la-methode-bopa/" target="_blank" rel="noopener">La Chaire Innovation BOPA&lt;/a> at Paul Brousse Hospital. I worked under the supervision of &lt;strong>Prof. Nguyen Huyen, Prof. Christian Sandor, and Dr Clément Cormi (BOPA).&lt;/strong> Our focus was on using augmented reality to visualize 3D imaging during open liver surgery.&lt;/p>
&lt;p>During my time in the lab, I spent time at the hospital observing surgeries and designing experimental protocols to test how different ways of presenting 3D images impact surgical performance and communication. I also developed a demo app to present 3D reconstructions of the liver using the &lt;strong>Canon MREAL HMD,&lt;/strong> which was later tested by surgeons. We also collaborated with &lt;a href="https://www.aphp.fr/actualite/rd-et-innovation-numerique-inauguration-du-tiers-lieu-dexperimentation-bopex-et-de-la" target="_blank" rel="noopener">PRIM 3D&lt;/a>, a 3D printing lab, to create life-sized silicone liver models, which were later used in the experiment.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/featured.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/featured_huaff070a83bf71c89d59f05c551c812e0_251234_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="featured.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo2.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo2_hubd6b7ac7b530ab84125276b4bbb949e6_6256781_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo2.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo3.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo3_huc38a946bdbae38917cdcb544694227af_186847_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo3.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo4.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo4_hu728fb2c6a3662898b26ddb53355c097f_246460_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo4.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>The project was a challenging yet rewarding experience that gave me valuable insight into how interdisciplinary collaboration between medical and technical expertise can lead to innovative solutions. It showed me how teamwork can help tackle complex tasks and contribute to improving healthcare.&lt;/p>
&lt;p>I was also lucky to attend the &lt;a href="https://rjc2024.afihm.org/" target="_blank" rel="noopener">Young Researchers in Human-Computer Interaction Meeting&lt;/a> in La Rochelle and volunteer at the &lt;a href="https://sui.acm.org/" target="_blank" rel="noopener">SUI Symposium on Spatial User Interaction&lt;/a>. Both events introduced me to new ideas, cutting-edge research, and inspiring people in the field of HCI.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo5.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo5_hu836e508bfc60138b77adb659ac13436a_298077_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo5.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo6.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo6_huaa7d7bb360aba8a5b2677d5c5255296d_189797_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo6.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo7.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo7_hua8237388394b671d2702cf34d6092f40_288679_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo7.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>But my internship wasn’t just about research! I met so many interesting people in the lab and had some great experiences outside the lab too. We organized fun outings like a trip to &lt;strong>Asterix Park&lt;/strong>, or &lt;strong>ice skating&lt;/strong>, which helped me feel more connected to the team.&lt;/p>
&lt;p>Looking back, my time with the ARAI Team was a valuable learning experience, both professionally and personally. I’m grateful for the support of my supervisors and colleagues. This experience has shaped my future, and I’m excited to see where it leads next!&lt;/p></description></item><item><title>Mizuki's Internship Report</title><link>https://ar-ai.org/post/25-01-09-mizuki-internship/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-01-09-mizuki-internship/</guid><description>&lt;p>Hello, this is Mizuki Akiyama. I’m a master’s student of Ochanomizu University in Japan. I joined team ARAI as an intern for about 2 months, from early October to mid December 2024. Not only did I proceed with my project, but I also met amazing people, so I was able to spend quality time. I will continue to work with team ARAI remotely from Tokyo!&lt;/p>
&lt;p>Before coming, I had some online meetings with Professor Christian. Since we talked about how we would do the project during the meetings, I started the implementation immediately upon joining. I could focus very well on my project. The project is based on my personal experience as a cheerleader; I want to help cheerleaders to improve their training using XR technologies! Our ultimate goal is to do it with AR, but as a first step, I built a prototype with VR&lt;/p>
&lt;p>Furthermore, we went out for dinner together in Paris, and on another day, I was shown around the city. I had a great time with everyone during the other moments.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_0012.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_0012_hu5036c615b2d6d6160c522dbc8854dc34_2232972_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_0012.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_5985.JPG" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_5985_hu2842f9bb582e8041cbb59fd7aefbf61c_881809_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_5985.JPG" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_6636.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_6636_hu259fbd5d4a4bab4623402aeb74037ee4_3025976_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_6636.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_7524.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_7524_hufa1113f165e35bfca06bf8896125f364_3604244_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7524.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>We had French Japanese food. I tried something called sushi, but it was not real sushi.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/mid" href="https://ar-ai.org/media/albums/mizuki/mid/IMG_8760.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/mid/IMG_8760_hu4157f350403e0a1d8ea16851b986a15f_3311829_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8760.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/mid" href="https://ar-ai.org/media/albums/mizuki/mid/IMG_8763.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/mid/IMG_8763_hu27d33409c024a14810ced22d8c747a02_3363074_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8763.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>Additionally, I visited Paris every weekend and I had some trips in Europe. I found it fascinating to experience the different vibes of each country.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_6559.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_6559_hu35556bad1ec0220329066c501ea585f0_2370521_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_6559.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7092.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7092_hua067d1554babbfdaa92618774b6fb548_2740686_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7092.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7822.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7822_huf1da7e5c8770ea7ccf4d5ef08b3b9faa_3810084_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7822.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8153.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8153_hu88e39909393733ab8ee350fb1bc5886c_2029441_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8153.jpg" width="422" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8287.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8287_huf704d76092f163b08300187d5403f133_2160445_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8287.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_9163.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_9163_hud7a28246445955b2ecf8281f70c59815_2376688_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_9163.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>Even though my English is not good, everyone was very welcoming. I’m so glad to experience this internship, and I appreciate their kindness.
Please come visit Japan! Thank you very much, ありがとう！&lt;/p></description></item><item><title>Postdoc Position Available</title><link>https://ar-ai.org/post/25-01-07-offer/</link><pubDate>Tue, 07 Jan 2025 14:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-01-07-offer/</guid><description/></item><item><title>Visit by Canon Japan</title><link>https://ar-ai.org/post/24-10-18-canon/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/24-10-18-canon/</guid><description>&lt;p>Canon Japan visited us to present their AR strategy, including their &lt;a href="https://www.youtube.com/watch?v=nweBR5WqfXc" target="_blank" rel="noopener">current&lt;/a> and &lt;a href="https://www.youtube.com/watch?v=tGu8NrbsdTA" target="_blank" rel="noopener">future&lt;/a> headsets. They also showed several really nice demos. It was fun as you can see in the image gallery below. You can also see &lt;a href="https://pages.saclay.inria.fr/emmanuel.pietriga/" target="_blank" rel="noopener">Emmanuel Pietriga&lt;/a>, leader of team ILDA, in some of the photos.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/20241018_152019.jpg" >
&lt;img src="https://ar-ai.org/media/albums/canon/20241018_152019_hu8503611eae5897e9f0d72a94c46d3faf_4262164_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20241018_152019.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/20241018_154232.jpg" >
&lt;img src="https://ar-ai.org/media/albums/canon/20241018_154232_hud8432aaedcb9b2ca6d7b408b22ad5550_3114318_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20241018_154232.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_7348.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_7348_hu52bd710e8ca60db26b29f823d4d9c7a4_2133758_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7348.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8101.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8101_huc543c82f445acaacf58792eb39a9acaa_3175872_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8101.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8102.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8102_hudbfc9af82186df3061d80d5229ec3f20_2186522_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8102.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8122.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8122_hu6954a591a76218e88bd057a11448f564_2576175_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8122.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8153.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8153_hue48178c84d2f4c816f62598c33ce033a_2905880_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8153.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8160.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8160_huffdf9126b1aaadde1cc5d5af5815d7fc_3418643_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8160.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8161.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8161_huca5c07074b9069afa875cff959cedfa4_2815767_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8161.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8164.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8164_hue79d26b5767d2df264b2202e8af9ff56_2722109_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8164.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;/div></description></item><item><title>Team ARAI presents at ACM SIGGRAPH 2024 Real-Time Live!</title><link>https://ar-ai.org/post/24-09-11-rtl/</link><pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/24-09-11-rtl/</guid><description>&lt;p>At the end of July 2024, we showed a demonstration at the premiere computer
graphics conference: &lt;em>ACM SIGGRAPH&lt;/em>. We even managed to get into one of their
most challenging categories: &lt;em>Real-Time Live!&lt;/em>&lt;/p>
&lt;p>Click to read Chris&amp;rsquo;s detailled experience report.&lt;/p></description></item><item><title>Contact</title><link>https://ar-ai.org/contact/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://ar-ai.org/contact/</guid><description/></item><item><title>People</title><link>https://ar-ai.org/people/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://ar-ai.org/people/</guid><description/></item><item><title>Tour</title><link>https://ar-ai.org/tour/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://ar-ai.org/tour/</guid><description/></item><item><title/><link>https://ar-ai.org/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ar-ai.org/admin/config.yml</guid><description/></item><item><title>Job Offers</title><link>https://ar-ai.org/offers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ar-ai.org/offers/</guid><description>&lt;p>We are always looking for excellent candidates (Postdocs, PhD students, interns) to expand our team. To express your interest, please send your application materials to &lt;a href="https://ar-ai.org/author/christian-sandor">Christian Sandor&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>CV&lt;/li>
&lt;li>Transcript of Records&lt;/li>
&lt;li>Link to portfolio (github, personal homepage, etc.)&lt;/li>
&lt;/ul>
&lt;h4 id="research-environment">Research Environment&lt;/h4>
&lt;p>The Laboratoire Interdisciplinaire des Sciences du Numérique (&lt;a href="https://www.lisn.upsaclay.fr/?lang=en" target="_blank" rel="noopener">LISN&lt;/a>) is a multidisciplinary laboratory at France&amp;rsquo;s leading research university: &lt;a href="https://en.wikipedia.org/wiki/Paris-Saclay_University#University_rankings" target="_blank" rel="noopener">Université Paris-Saclay&lt;/a>. The successful candidate will be expected to join team ARAI within LISN’s HCI department (&lt;a href="https://www.lisn.upsaclay.fr/research/research-departments/interaction-with-human/?lang=en" target="_blank" rel="noopener">IaH&lt;/a>).&lt;/p>
&lt;p>Team ARAI was founded in May 2024 to pioneer the application of AI to AR. It is strongly connected to the large French projects &lt;a href="http://continuum.website" target="_blank" rel="noopener">CONTINUUM&lt;/a> and &lt;a href="https://pepr-ensemble.fr/home.html" target="_blank" rel="noopener">ENSEMBLE&lt;/a>.&lt;/p>
&lt;h4 id="desired-skills">Desired Skills&lt;/h4>
&lt;ul>
&lt;li>AR/VR&lt;/li>
&lt;li>HCI/3DUIs&lt;/li>
&lt;li>Psychology/Neuroscience&lt;/li>
&lt;li>Computer Graphics&lt;/li>
&lt;li>Computer Vision&lt;/li>
&lt;li>Deep Learning&lt;/li>
&lt;/ul>
&lt;p>We also consider candidates who are outside of this profile, but who can convince us that they can grow into it.&lt;/p>
&lt;h2 id="postdocs">Postdocs&lt;/h2>
&lt;h4 id="summary">Summary&lt;/h4>
&lt;ul>
&lt;li>Starting date: continuously&lt;/li>
&lt;li>Salary: up to EUR 4291 (gross per month; depending on experience)&lt;/li>
&lt;/ul>
&lt;h4 id="introduction">Introduction&lt;/h4>
&lt;p>Generative Artiﬁcial Intelligence (AI) and Augmented Reality (AR) have made incredible progress over the last few years. We are not far away from a future, in which a user&amp;rsquo;s view could be continuously modified by their smart glasses. Besides some obvious ethical concerns about such a future, there are also immense opportunities for using this capability to make our lives better. The high-level context of this position is to help us develop fundamental technologies and study their applications and effects on humans.&lt;/p>
&lt;p>A special benefit of this position is that is intended to support outstanding young researchers to prepare an application for a lifetime position at &lt;a href="https://en.wikipedia.org/wiki/French_National_Centre_for_Scientific_Research" target="_blank" rel="noopener">CNRS&lt;/a>. These positions are unique, as they provide the probably fastest tracks for talented young researchers to get a tenured position (more than 5 years faster than a comparable track in e.g. the USA). CNRS positions also come with a status as French civil servant, which includes significant benefits (e.g. sabbaticals with a duration of up to 8 years, true lifetime employment, an attractive pension, etc.).&lt;/p>
&lt;p>The ideal candidate has 2-3 years of postdoc experience.&lt;/p></description></item></channel></rss>