<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>News | Team ARAI</title><link>https://ar-ai.org/post/</link><atom:link href="https://ar-ai.org/post/index.xml" rel="self" type="application/rss+xml"/><description>News</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 13 Feb 2026 00:00:00 +0000</lastBuildDate><image><url>https://ar-ai.org/media/icon_huf3b78d476b7845bee05ab0b9b7b31b8f_51310_512x512_fill_lanczos_center_3.png</url><title>News</title><link>https://ar-ai.org/post/</link></image><item><title>Cristian's PhD Defense</title><link>https://ar-ai.org/post/26-13-02-cristian-defense/</link><pubDate>Fri, 13 Feb 2026 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/26-13-02-cristian-defense/</guid><description>&lt;p>I am pleased to invite you to the defense of my PhD thesis titled:&lt;/p>
&lt;p>&lt;strong>Enhancing Human Performance Through Augmented Vision&lt;/strong>&lt;/p>
&lt;p>supervised by: Professeur Christian SANDOR, Maître de conférences Richard LEGRAS et Maître de conférences Marie-Anne BURCKLEN.&lt;/p>
&lt;p>The defense will be held in English and will take place on Monday, February 16th, 2026 at 14:00.&lt;/p>
&lt;h2 id="location">Location:&lt;/h2>
&lt;p>Room: Salle Conférences - Campus Universitaire bât.507 - Rue du Belvédère 91405 - Orsay.&lt;/p>
&lt;p>Online: &lt;a href="https://bbb.lisn.upsaclay.fr/rooms/n3k-chl-cpk-ghs/join" target="_blank" rel="noopener">https://bbb.lisn.upsaclay.fr/rooms/n3k-chl-cpk-ghs/join&lt;/a>&lt;/p>
&lt;h2 id="jury">Jury:&lt;/h2>
&lt;ul>
&lt;li>M. Vincent NOURRIT, Professeur, IMT Atlantique, Rapporteur&lt;/li>
&lt;li>Mme Stefanie ZOLLMANN, Associate Professor, Aarhus University, Rapporteure&lt;/li>
&lt;li>M. Laurent VABRE, Docteur, Imagine Eyes, Examinateur&lt;/li>
&lt;li>M. Gabriele FACCIOLO, Professeur, ENS Paris-Saclay, Examinateur&lt;/li>
&lt;/ul>
&lt;h2 id="keywords">Keywords:&lt;/h2>
&lt;p>Augmented Vision, Human-Computer Interaction, Human Visual System, Freeform Lenses, Spatial Light Modulator, Computational Optics&lt;/p>
&lt;h2 id="abstract">Abstract:&lt;/h2>
&lt;p>Augmented Vision (AV) is an emerging field that aims to enhance human sight by directly modulating a user&amp;rsquo;s view of the real world. Unlike traditional visual aids such as spectacles or contact lenses that provide static correction, AV systems offer dynamic, real-time modulation of the real world. This capability to actively change the user&amp;rsquo;s view opens up new possibilities for both correcting and augmenting human vision. One of the three most common applications for AV is vision correction. This is particularly relevant for presbyopia, an age-related condition affecting 1.8 billion people worldwide. Conventional solutions, such as progressive glasses or multifocal contact lenses, offer a static compromise with fixed focal zones or averaged focus, respectively. In contrast, AV systems use their inherently dynamic behaviour to address these limitations. For instance, some systems use automatic single focus approaches using focus-tunable lenses to focus at specific depths. However, this approach is limited to applying one optical power across the entire field of view at a time. The alternatives are multifocal systems, which provide granular, spatial control over focus. However, while the concept has been demonstrated, there is a lack of objective evaluation of the imaging quality of such systems.&lt;/p>
&lt;p>This thesis addresses two primary objectives. The first is to establish a conceptual framework for Augmented Vision by stating a formal definition and a new taxonomy that categorises existing systems by their light modulation method. The second is to address a gap identified by this framework: the lack of multifocal augmented vision systems and the evaluation of their imaging performance. To tackle the challenges of providing multifocal vision, this thesis proposes an optical architecture that combines a phase-only Spatial Light Modulator (SLM) with a Lohmann lens, allowing spatial control of the optical power over the Field Of View (FOV), providing see-through, real-world modulation. We validate the prototype by measuring the Modulation Transfer Function (MTF) for global and multifocal correction, placing objects at various depths and setting the system&amp;rsquo;s focus at them. We use a camera with its focus set to infinity to simulate a presbyopic eye during the measurements. The results confirm that the system can bring objects from multiple depths into focus, demonstrating a higher MTF for each focused plane when the system is active compared to the inactive condition.&lt;/p>
&lt;p>This thesis provides two primary contributions: (1) a formal conceptual framework for AV systems that identifies the research gap, and (2) a prototype of an optical platform for multifocal vision and its evaluation. This platform serves as foundational technology for exploring a new class of interfaces based on spatially-variant optical power to enhance visual perception and address widespread visual impairments.&lt;/p></description></item><item><title>My Experience at ADOS: From Keynotes to Hackathon Experiments</title><link>https://ar-ai.org/post/25-03-28-ados/</link><pubDate>Tue, 02 Sep 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-03-28-ados/</guid><description>&lt;p>On &lt;strong>28 March 2025&lt;/strong>, I had the pleasure of participating in &lt;a href="https://www.lightricks.com/ados" target="_blank" rel="noopener">ADOS&lt;/a> in Paris—a creative technology event—co‑organized by &lt;a href="https://banodoco.ai" target="_blank" rel="noopener">Banodoco&lt;/a> and &lt;a href="https://www.lightricks.com" target="_blank" rel="noopener">Lightricks&lt;/a> at &lt;a href="https://www.artifex-lab.com" target="_blank" rel="noopener">Artifex Lab&lt;/a>. Walking into that space, you could feel the excitement: artists, engineers, and technologists converging around the open‑source &lt;a href="https://github.com/Lightricks/LTX-Video" target="_blank" rel="noopener">LTXV&lt;/a> model. I’d been following LTXV’s rapid evolution—a text-to-video model that generates video clips from text in real time—so it was thrilling to see the community around it in action.&lt;/p>
&lt;h2 id="first-day-at-ados-talks--connections">First Day at ADOS: Talks &amp;amp; Connections&lt;/h2>
&lt;p>In the evening’s lineup, standout presentations ranged from &lt;a href="https://www.youtube.com/live/gBeZSbaxMvc?t=468s" target="_blank" rel="noopener">Emma Catnip&lt;/a>’s soulful art pieces to &lt;a href="https://github.com/yvann-ba" target="_blank" rel="noopener">Vibeke Bertelsen&lt;/a>’s uncanny human forms, and Yvann Barbot’s ComfyUI audio-reactive video demos. My supervisor, &lt;a href="https://drsandor.net" target="_blank" rel="noopener">Christian Sandor&lt;/a>, also delivered an inspiring keynote, and our team showcased a &lt;a href="https://youtu.be/gBeZSbaxMvc?t=7066" target="_blank" rel="noopener">demo&lt;/a> exploring the convergence of AR and AI—a project that deeply influenced my own thinking about generative storytelling.&lt;/p>
&lt;p>Amid all this, I found LTXV’s open‑source availability fascinating—not just as technical innovation, but as a platform for collective creativity. Seeing Lightricks being open with their weights and tools seemed like a pivotal moment in democratizing video AI.&lt;/p>
&lt;h2 id="day-two-hackathon--video-editing-and-reference-based-styletransfer-experiments">Day Two: Hackathon &amp;amp; Video Editing and Reference-based Style‑Transfer Experiments&lt;/h2>
&lt;p>The very next day I joined the ADOS hackathon, working side by side with others to push LTXV’s limits. Provided computation resources, power, snacks, and steady energy pulses fueled the intense collaborative rhythm of the event.
For my project, I focused on one core idea: simultaneous style transfer and editing for text‑to‑video (T2V) using LTXV. Given a style reference image, a video clip, and a text prompt describing the edit, I experimented with three distinct methods to transfer visual style while&lt;/p>
&lt;ol>
&lt;li>Latent alpha blending: Alpha blending between the inverted style image latent and the inverted latent of the video&amp;rsquo;s first frame.&lt;/li>
&lt;li>KV sharing with alpha blending: Repeating the style’s keys and values to match video dimensions (repeat-interleave) and blending them.&lt;/li>
&lt;li>KV sharing via concatenation: Appending the style&amp;rsquo;s keys and values to the video&amp;rsquo;s keys and values for joint processing with full 3D attention.&lt;/li>
&lt;/ol>
&lt;figure>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 60%; margin-bottom:0;">
&lt;video style="margin: 0" controls autoplay muted loop>
&lt;source src="media/input_hf.mp4" type="video/mp4">
Your browser does not support video.
&lt;/video>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Input Video &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 40%; margin-bottom:0;">
&lt;img src="media/style_image.png"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Reference Style &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 50%; margin-bottom:0;">
&lt;img src="media/algo_1.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 1 &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 50%; margin-bottom:0;">
&lt;img src="media/algo_2.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 2 &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;div style="display: flex; align-items: center; gap: 1rem;">
&lt;figure style="width: 50%; margin-bottom: 10px;">
&lt;img src="media/algo_3_1.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 3 (Layer 5) &lt;/figcaption>
&lt;/figure>
&lt;figure style="width: 50%; margin-bottom: 10px;">
&lt;img src="media/algo_3_2.gif"/>
&lt;figcaption style="text-align: center; font-style: italic; margin-bottom:0;"> Method 3 (Layer 27) &lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;p style="font-size: 12px"> Text Prompt: A big horse triumphantly at the peak of a towering mountain. Panorama of rugged peaks and valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, moons, but the remainder of the scene is mostly realistic.&lt;/p>
&lt;figcaption style="text-align: left; font-style: italic;"> &lt;b>Figure 1:&lt;/b> Method 1 fails to propagate style information to the generated video. In Method 3, the choice of attention layer for injection critically impacts the outcome: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, whereas later-layer injection (e.g., layer 27) achieves a more balanced integration of style and motion. Style image from &lt;a href="https://www.pinterest.com">Pinterest.&lt;/a> Input video from &lt;a href="https://huggingface.co">Hugging Face.&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;p>For the text‑based editing component, I used &lt;a href="https://rf-solver-edit.github.io" target="_blank" rel="noopener">RF‑Edit&lt;/a>. My findings indicate that latent alpha blending is ineffective for style transfer. KV sharing methods can propagate style, but often at the expense of content fidelity or motion consistency. In particular, style injection may introduce unnatural motion not present in the original video. The injection point within the attention layers significantly affects results: early-layer injection (e.g., layers 5 or 13) suppresses motion and overemphasizes style, while later-layer injection (e.g., layer 27) offers a better balance between style and motion.&lt;/p>
&lt;h2 id="reflections--what-comes-next">Reflections &amp;amp; What Comes Next&lt;/h2>
&lt;p>ADOS was an inspiring gathering—not only for witnessing the technical evolution of generative video, but also for experiencing the vibrant culture of open, collaborative experimentation that surrounds it.
Huge thanks to the ADOS team—&lt;strong>Banodoco, Lightricks, and Artifex Lab&lt;/strong>—for organizing an inspiring and technically rich weekend. Events like this make open‑source generative video feel alive. I can’t wait to keep building on what started in Paris.&lt;/p></description></item><item><title>Zofia's Internship Report</title><link>https://ar-ai.org/post/25-02-18-zofia-internship/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-02-18-zofia-internship/</guid><description>&lt;p>Hi, my name is Zofia. Originally from Poland, I am a &lt;strong>M2 Master’s student at Université Paris Cité&lt;/strong>.&lt;/p>
&lt;p>From March 2023 to January 2024, I had the opportunity to be part of the &lt;strong>ARAI Team&lt;/strong> as an intern. I joined the lab to work on a project in collaboration with &lt;a href="https://www.chaire-bopa.fr/la-methode-bopa/" target="_blank" rel="noopener">La Chaire Innovation BOPA&lt;/a> at Paul Brousse Hospital. I worked under the supervision of &lt;strong>Prof. Nguyen Huyen, Prof. Christian Sandor, and Dr Clément Cormi (BOPA).&lt;/strong> Our focus was on using augmented reality to visualize 3D imaging during open liver surgery.&lt;/p>
&lt;p>During my time in the lab, I spent time at the hospital observing surgeries and designing experimental protocols to test how different ways of presenting 3D images impact surgical performance and communication. I also developed a demo app to present 3D reconstructions of the liver using the &lt;strong>Canon MREAL HMD,&lt;/strong> which was later tested by surgeons. We also collaborated with &lt;a href="https://www.aphp.fr/actualite/rd-et-innovation-numerique-inauguration-du-tiers-lieu-dexperimentation-bopex-et-de-la" target="_blank" rel="noopener">PRIM 3D&lt;/a>, a 3D printing lab, to create life-sized silicone liver models, which were later used in the experiment.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/featured.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/featured_huaff070a83bf71c89d59f05c551c812e0_251234_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="featured.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo2.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo2_hubd6b7ac7b530ab84125276b4bbb949e6_6256781_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo2.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo3.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo3_huc38a946bdbae38917cdcb544694227af_186847_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo3.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/top" href="https://ar-ai.org/media/albums/zofia/top/photo4.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/top/photo4_hu728fb2c6a3662898b26ddb53355c097f_246460_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo4.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>The project was a challenging yet rewarding experience that gave me valuable insight into how interdisciplinary collaboration between medical and technical expertise can lead to innovative solutions. It showed me how teamwork can help tackle complex tasks and contribute to improving healthcare.&lt;/p>
&lt;p>I was also lucky to attend the &lt;a href="https://rjc2024.afihm.org/" target="_blank" rel="noopener">Young Researchers in Human-Computer Interaction Meeting&lt;/a> in La Rochelle and volunteer at the &lt;a href="https://sui.acm.org/" target="_blank" rel="noopener">SUI Symposium on Spatial User Interaction&lt;/a>. Both events introduced me to new ideas, cutting-edge research, and inspiring people in the field of HCI.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo5.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo5_hu836e508bfc60138b77adb659ac13436a_298077_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo5.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo6.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo6_huaa7d7bb360aba8a5b2677d5c5255296d_189797_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo6.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-zofia/bottom" href="https://ar-ai.org/media/albums/zofia/bottom/photo7.jpg" >
&lt;img src="https://ar-ai.org/media/albums/zofia/bottom/photo7_hua8237388394b671d2702cf34d6092f40_288679_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="photo7.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>But my internship wasn’t just about research! I met so many interesting people in the lab and had some great experiences outside the lab too. We organized fun outings like a trip to &lt;strong>Asterix Park&lt;/strong>, or &lt;strong>ice skating&lt;/strong>, which helped me feel more connected to the team.&lt;/p>
&lt;p>Looking back, my time with the ARAI Team was a valuable learning experience, both professionally and personally. I’m grateful for the support of my supervisors and colleagues. This experience has shaped my future, and I’m excited to see where it leads next!&lt;/p></description></item><item><title>Mizuki's Internship Report</title><link>https://ar-ai.org/post/25-01-09-mizuki-internship/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-01-09-mizuki-internship/</guid><description>&lt;p>Hello, this is Mizuki Akiyama. I’m a master’s student of Ochanomizu University in Japan. I joined team ARAI as an intern for about 2 months, from early October to mid December 2024. Not only did I proceed with my project, but I also met amazing people, so I was able to spend quality time. I will continue to work with team ARAI remotely from Tokyo!&lt;/p>
&lt;p>Before coming, I had some online meetings with Professor Christian. Since we talked about how we would do the project during the meetings, I started the implementation immediately upon joining. I could focus very well on my project. The project is based on my personal experience as a cheerleader; I want to help cheerleaders to improve their training using XR technologies! Our ultimate goal is to do it with AR, but as a first step, I built a prototype with VR&lt;/p>
&lt;p>Furthermore, we went out for dinner together in Paris, and on another day, I was shown around the city. I had a great time with everyone during the other moments.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_0012.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_0012_hu5036c615b2d6d6160c522dbc8854dc34_2232972_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_0012.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_5985.JPG" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_5985_hu2842f9bb582e8041cbb59fd7aefbf61c_881809_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_5985.JPG" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_6636.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_6636_hu259fbd5d4a4bab4623402aeb74037ee4_3025976_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_6636.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/top" href="https://ar-ai.org/media/albums/mizuki/top/IMG_7524.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/top/IMG_7524_hufa1113f165e35bfca06bf8896125f364_3604244_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7524.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>We had French Japanese food. I tried something called sushi, but it was not real sushi.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/mid" href="https://ar-ai.org/media/albums/mizuki/mid/IMG_8760.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/mid/IMG_8760_hu4157f350403e0a1d8ea16851b986a15f_3311829_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8760.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/mid" href="https://ar-ai.org/media/albums/mizuki/mid/IMG_8763.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/mid/IMG_8763_hu27d33409c024a14810ced22d8c747a02_3363074_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8763.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>Additionally, I visited Paris every weekend and I had some trips in Europe. I found it fascinating to experience the different vibes of each country.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_6559.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_6559_hu35556bad1ec0220329066c501ea585f0_2370521_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_6559.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7092.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7092_hua067d1554babbfdaa92618774b6fb548_2740686_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7092.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7822.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_7822_huf1da7e5c8770ea7ccf4d5ef08b3b9faa_3810084_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7822.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8153.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8153_hu88e39909393733ab8ee350fb1bc5886c_2029441_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8153.jpg" width="422" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8287.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_8287_huf704d76092f163b08300187d5403f133_2160445_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8287.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-mizuki/bottom" href="https://ar-ai.org/media/albums/mizuki/bottom/IMG_9163.jpg" >
&lt;img src="https://ar-ai.org/media/albums/mizuki/bottom/IMG_9163_hud7a28246445955b2ecf8281f70c59815_2376688_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_9163.jpg" width="563" height="750">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;p>Even though my English is not good, everyone was very welcoming. I’m so glad to experience this internship, and I appreciate their kindness.
Please come visit Japan! Thank you very much, ありがとう！&lt;/p></description></item><item><title>Postdoc Position Available</title><link>https://ar-ai.org/post/25-01-07-offer/</link><pubDate>Tue, 07 Jan 2025 14:00:00 +0000</pubDate><guid>https://ar-ai.org/post/25-01-07-offer/</guid><description/></item><item><title>Visit by Canon Japan</title><link>https://ar-ai.org/post/24-10-18-canon/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/24-10-18-canon/</guid><description>&lt;p>Canon Japan visited us to present their AR strategy, including their &lt;a href="https://www.youtube.com/watch?v=nweBR5WqfXc" target="_blank" rel="noopener">current&lt;/a> and &lt;a href="https://www.youtube.com/watch?v=tGu8NrbsdTA" target="_blank" rel="noopener">future&lt;/a> headsets. They also showed several really nice demos. It was fun as you can see in the image gallery below. You can also see &lt;a href="https://pages.saclay.inria.fr/emmanuel.pietriga/" target="_blank" rel="noopener">Emmanuel Pietriga&lt;/a>, leader of team ILDA, in some of the photos.&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/20241018_152019.jpg" >
&lt;img src="https://ar-ai.org/media/albums/canon/20241018_152019_hu8503611eae5897e9f0d72a94c46d3faf_4262164_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20241018_152019.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/20241018_154232.jpg" >
&lt;img src="https://ar-ai.org/media/albums/canon/20241018_154232_hud8432aaedcb9b2ca6d7b408b22ad5550_3114318_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20241018_154232.jpg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_7348.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_7348_hu52bd710e8ca60db26b29f823d4d9c7a4_2133758_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_7348.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8101.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8101_huc543c82f445acaacf58792eb39a9acaa_3175872_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8101.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8102.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8102_hudbfc9af82186df3061d80d5229ec3f20_2186522_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8102.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8122.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8122_hu6954a591a76218e88bd057a11448f564_2576175_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8122.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8153.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8153_hue48178c84d2f4c816f62598c33ce033a_2905880_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8153.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8160.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8160_huffdf9126b1aaadde1cc5d5af5815d7fc_3418643_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8160.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8161.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8161_huca5c07074b9069afa875cff959cedfa4_2815767_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8161.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-canon" href="https://ar-ai.org/media/albums/canon/IMG_8164.jpeg" >
&lt;img src="https://ar-ai.org/media/albums/canon/IMG_8164_hue79d26b5767d2df264b2202e8af9ff56_2722109_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_8164.jpeg" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;/div></description></item><item><title>Team ARAI presents at ACM SIGGRAPH 2024 Real-Time Live!</title><link>https://ar-ai.org/post/24-09-11-rtl/</link><pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate><guid>https://ar-ai.org/post/24-09-11-rtl/</guid><description>&lt;p>At the end of July 2024, we showed a demonstration at the premiere computer
graphics conference: &lt;em>ACM SIGGRAPH&lt;/em>. We even managed to get into one of their
most challenging categories: &lt;em>Real-Time Live!&lt;/em>&lt;/p>
&lt;p>Click to read Chris&amp;rsquo;s detailled experience report.&lt;/p></description></item></channel></rss>